from ultralytics import YOLO
model = YOLO("yolov8n.pt")
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo'
os.environ['TORCHDYNAMO_DISABLE'] = '1'
os.environ['WANDB_MODE'] = 'disabled'
import warnings
warnings.filterwarnings('ignore')
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
import random
from pathlib import Path
import pandas as pd
import shutil
from sklearn.model_selection import train_test_split
import yaml
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import subprocess
from ultralytics import YOLO, settings
import time
import traceback
import zipfile
import json

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def convert_json_to_yolo_format(json_ann_dir, img_dir, output_ann_dir):
    """Convert JSON annotations to YOLO txt format for the specific format provided"""
    json_ann_dir = Path(json_ann_dir)
    img_dir = Path(img_dir)
    output_ann_dir = Path(output_ann_dir)
    output_ann_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Converting JSON annotations from {json_ann_dir} to YOLO format in {output_ann_dir}")
    
    json_files = list(json_ann_dir.glob("*.json"))
    print(f"Found {len(json_files)} JSON annotation files")
    
    converted_count = 0
    
    for json_file in json_files:
        try:
            with open(json_file, 'r') as f:
                annotation = json.load(f)
            
            # Extract the number from annotation filename
            # ann_image_7.json -> 7
            ann_name = json_file.stem  # e.g., "ann_image_7"
            if 'ann_image_' in ann_name:
                image_number = ann_name.replace('ann_image_', '')
                # Look for corresponding train_image_X or test_image_X
                img_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
                img_file = None
                
                # Try both train_image_X and test_image_X patterns
                for prefix in ['train_image_', 'test_image_']:
                    for ext in img_extensions:
                        potential_img = img_dir / f"{prefix}{image_number}{ext}"
                        if potential_img.exists():
                            img_file = potential_img
                            break
                    if img_file:
                        break
                
                if not img_file:
                    print(f"Warning: No corresponding image found for {json_file.name} (looking for train_image_{image_number}.* or test_image_{image_number}.*)")
                    continue
            else:
                print(f"Warning: Unexpected annotation filename format: {json_file.name}")
                continue
            
            # Get image dimensions from JSON (more reliable than loading image)
            if 'size' in annotation:
                img_width = annotation['size']['width']
                img_height = annotation['size']['height']
            else:
                # Fallback to loading image
                img = cv2.imread(str(img_file))
                if img is None:
                    print(f"Warning: Could not read image {img_file}")
                    continue
                img_height, img_width = img.shape[:2]
            
            # Create YOLO format annotation
            yolo_annotations = []
            
            # Handle the specific JSON format provided
            if 'objects' in annotation:
                for obj in annotation['objects']:
                    if obj.get('geometryType') == 'rectangle' and 'points' in obj:
                        points = obj['points']['exterior']
                        if len(points) >= 2:
                            # Extract coordinates (top-left and bottom-right)
                            x1, y1 = points[0]
                            x2, y2 = points[1]
                            
                            # Calculate bbox (ensure correct order)
                            x = min(x1, x2)
                            y = min(y1, y2)
                            w = abs(x2 - x1)
                            h = abs(y2 - y1)
                            
                            # Convert to YOLO format (normalized center coordinates)
                            center_x = (x + w/2) / img_width
                            center_y = (y + h/2) / img_height
                            norm_width = w / img_width
                            norm_height = h / img_height
                            
                            # Class ID (signature class is 0)
                            class_id = 0
                            yolo_annotations.append(f"{class_id} {center_x:.6f} {center_y:.6f} {norm_width:.6f} {norm_height:.6f}")
            
            # Write YOLO format annotation file
            if yolo_annotations:
                # Use the image filename (without extension) for the output txt file
                img_base_name = img_file.stem  # e.g., "train_image_7" or "test_image_7"
                output_file = output_ann_dir / f"{img_base_name}.txt"
                with open(output_file, 'w') as f:
                    f.write('\n'.join(yolo_annotations))
                converted_count += 1
                print(f"Converted {json_file.name} -> {output_file.name} ({len(yolo_annotations)} objects)")
            else:
                print(f"Warning: No valid annotations found in {json_file.name}")
                
        except Exception as e:
            print(f"Error processing {json_file}: {e}")
    
    print(f"Successfully converted {converted_count} annotation files")
    return converted_count

class CEDARDatasetProcessor:
    def __init__(self, cedar_path="/kaggle/input/cedardataset/signatures"):
        self.cedar_path = Path(cedar_path)
        self.output_dir = Path("/kaggle/working/processed_cedar")
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "siamese_data").mkdir(exist_ok=True)
        (self.output_dir / "reference_signatures").mkdir(exist_ok=True)
        
    def process_cedar_dataset(self):
        print("Processing CEDAR dataset...")
        genuine_files = []
        forged_files = []
        
        for file_path in self.cedar_path.rglob("*.png"):
            filename = file_path.name.lower()
            parent_folder = file_path.parent.name.lower()
            
            if "genuine" in filename or "_g_" in filename or "genuine" in parent_folder:
                genuine_files.append(file_path)
            elif "forged" in filename or "_f_" in filename or "forged" in parent_folder or "forg" in parent_folder:
                forged_files.append(file_path)
            else:
                genuine_files.append(file_path)
        print(f"Found {len(genuine_files)} genuine signatures")
        print(f"Found {len(forged_files)} forged signatures")
        self.create_siamese_dataset(genuine_files, forged_files)
        self.create_reference_database(genuine_files)
        
    def create_siamese_dataset(self, genuine_files, forged_files):
        print("Creating Siamese dataset...")
        siamese_dir = self.output_dir / "siamese_data"
        person_signatures = {}
        
        for sig_file in genuine_files:
            filename = sig_file.name
            person_id = self.extract_person_id(filename)
            if person_id not in person_signatures:
                person_signatures[person_id] = {'genuine': [], 'forged': []}
            person_signatures[person_id]['genuine'].append(sig_file)
        
        for sig_file in forged_files:
            filename = sig_file.name
            person_id = self.extract_person_id(filename)
            if person_id not in person_signatures:
                person_signatures[person_id] = {'genuine': [], 'forged': []}
            person_signatures[person_id]['forged'].append(sig_file)
        
        self.create_signature_pairs(person_signatures, siamese_dir)
        
    def extract_person_id(self, filename):
        import re
        match = re.search(r'(\d{3})', filename)
        if match:
            return int(match.group(1))
        return hash(filename) % 100
        
    def create_signature_pairs(self, person_signatures, output_dir):
        pairs_data = []
        
        for person_id, signatures in person_signatures.items():
            genuine_sigs = signatures['genuine']
            if len(genuine_sigs) >= 2:
                for i in range(len(genuine_sigs)):
                    for j in range(i + 1, min(i + 5, len(genuine_sigs))):
                        pairs_data.append({
                            'image1': str(genuine_sigs[i]),
                            'image2': str(genuine_sigs[j]),
                            'label': 1,
                            'person_id': person_id
                        })
        
        person_ids = list(person_signatures.keys())
        for person_id in person_ids:
            genuine_sigs = person_signatures[person_id]['genuine']
            forged_sigs = person_signatures[person_id]['forged']
            
            # Balance positive and negative pairs
            num_positive = sum(1 for p in pairs_data if p['label'] == 1 and p['person_id'] == person_id)
            num_negative = 0
            
            for genuine in genuine_sigs[:3]:
                for forged in forged_sigs[:min(2, len(forged_sigs))]:
                    if num_negative < num_positive:
                        pairs_data.append({
                            'image1': str(genuine),
                            'image2': str(forged),
                            'label': 0,
                            'person_id': person_id
                        })
                        num_negative += 1
            
            other_persons = [pid for pid in person_ids if pid != person_id]
            for genuine in genuine_sigs[:3]:
                for other_person in random.sample(other_persons, min(2, len(other_persons))):
                    if num_negative < num_positive:
                        other_genuine = person_signatures[other_person]['genuine']
                        if other_genuine:
                            pairs_data.append({
                                'image1': str(genuine),
                                'image2': str(random.choice(other_genuine)),
                                'label': 0,
                                'person_id': f"{person_id}_vs_{other_person}"
                            })
                            num_negative += 1
        
        pairs_df = pd.DataFrame(pairs_data)
        pairs_df.to_csv(output_dir / "signature_pairs.csv", index=False)
        print(f"Created {len(pairs_data)} signature pairs")
        print(f"Positive pairs: {len(pairs_df[pairs_df['label'] == 1])}")
        print(f"Negative pairs: {len(pairs_df[pairs_df['label'] == 0])}")
        return pairs_data
        
    def create_reference_database(self, genuine_files):
        print("Creating reference signature database...")
        ref_dir = self.output_dir / "reference_signatures"
        person_refs = {}
        
        for sig_file in genuine_files:
            person_id = self.extract_person_id(sig_file.name)
            if person_id not in person_refs:
                person_refs[person_id] = sig_file
        
        for person_id, sig_file in person_refs.items():
            try:
                sig_img = cv2.imread(str(sig_file))
                if sig_img is not None:
                    ref_path = ref_dir / f"person_{person_id:03d}_ref.png"
                    cv2.imwrite(str(ref_path), sig_img)
            except Exception as e:
                print(f"Error processing reference for person {person_id}: {e}")
        
        print(f"Created reference signatures for {len(person_refs)} persons")

class SiameseNetwork(nn.Module):
    def __init__(self, input_dim=224):
        super(SiameseNetwork, self).__init__()
        self.backbone = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=10, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(128, 256, kernel_size=4, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        self._calculate_conv_output_size(input_dim)
        self.fc_layers = nn.Sequential(
            nn.Linear(self.conv_output_size, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(1024, 512)  # Increased to 512 from 256
        )
        self.classifier = nn.Sequential(
            nn.Linear(512, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )
        
    def _calculate_conv_output_size(self, input_dim):
        with torch.no_grad():
            x = torch.randn(1, 1, input_dim, input_dim)
            x = self.backbone(x)
            self.conv_output_size = x.view(1, -1).size(1)
            
    def forward_one(self, x):
        x = self.backbone(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x
        
    def forward(self, input1, input2):
        output1 = self.forward_one(input1)
        output2 = self.forward_one(input2)
        distance = F.pairwise_distance(output1, output2)
        diff = torch.abs(output1 - output2)
        classification = self.classifier(diff)
        return classification, distance

class SignaturePairsDataset(Dataset):
    def __init__(self, csv_path, transform=None):
        self.pairs_df = pd.read_csv(csv_path)
        self.transform = transform
        
    def __len__(self):
        return len(self.pairs_df)
        
    def __getitem__(self, idx):
        row = self.pairs_df.iloc[idx]
        try:
            img1 = Image.open(row['image1']).convert('L')
            img2 = Image.open(row['image2']).convert('L')
        except Exception as e:
            print(f"Error loading images at index {idx}: {e}")
            img1 = Image.new('L', (224, 224), 255)
            img2 = Image.new('L', (224, 224), 255)
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
        
        label = torch.tensor(row['label'], dtype=torch.float32)
        return img1, img2, label

class SiameseTrainer:
    def __init__(self, model, device=None, patience=5):
        self.model = model
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.criterion = nn.BCELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=2, verbose=True)
        self.train_losses = []
        self.val_accuracies = []
        self.best_accuracy = 0
        self.patience = patience
        self.patience_counter = 0
        self.best_val_loss = float('inf')
        
    def train_epoch(self, train_loader):
        self.model.train()
        total_loss = 0.0
        for batch_idx, (img1, img2, labels) in enumerate(tqdm(train_loader, desc="Training")):
            img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.to(self.device)
            self.optimizer.zero_grad()
            output, distance = self.model(img1, img2)
            loss = self.criterion(output.squeeze(), labels)
            loss.backward()
            self.optimizer.step()
            total_loss += loss.item()
        return total_loss / len(train_loader)
        
    def validate(self, val_loader):
        self.model.eval()
        val_loss = 0.0
        predictions = []
        true_labels = []
        with torch.no_grad():
            for img1, img2, labels in tqdm(val_loader, desc="Validation"):
                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.to(self.device)
                output, distance = self.model(img1, img2)
                loss = self.criterion(output.squeeze(), labels)
                val_loss += loss.item()
                pred = (output.squeeze() > 0.5).float()
                predictions.extend(pred.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        
        accuracy = accuracy_score(true_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary', zero_division=0)
        return val_loss / len(val_loader), accuracy, precision, recall, f1
        
    def train(self, train_loader, val_loader, epochs=30, save_dir="models"):
        save_dir = Path("/kaggle/working/") / save_dir
        save_dir.mkdir(exist_ok=True)
        
        for epoch in range(epochs):
            print(f"Epoch {epoch + 1}/{epochs}")
            train_loss = self.train_epoch(train_loader)
            val_loss, accuracy, precision, recall, f1 = self.validate(val_loader)
            self.scheduler.step(accuracy)
            
            self.train_losses.append(train_loss)
            self.val_accuracies.append(accuracy)
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}")
            print(f"Accuracy: {accuracy:.4f}")
            print(f"Precision: {precision:.4f}")
            print(f"Recall: {recall:.4f}")
            print(f"F1 Score: {f1:.4f}")
            
            if accuracy > self.best_accuracy:
                self.best_accuracy = accuracy
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_model(save_dir / "best_siamese_model.pth")
                print(f"New best model saved! Accuracy: {self.best_accuracy:.4f}")
            else:
                self.patience_counter += 1
                print(f"No improvement. Patience counter: {self.patience_counter}/{self.patience}")
            
            if self.patience_counter >= self.patience:
                print(f"Early stopping triggered after {self.patience_counter} epochs without improvement")
                break
            
            self.save_model(save_dir / "latest_siamese_model.pth")
        
        return self.train_losses, self.val_accuracies
    
    def test(self, test_loader):
        self.model.eval()
        predictions = []
        true_labels = []
        with torch.no_grad():
            for img1, img2, labels in tqdm(test_loader, desc="Testing"):
                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.to(self.device)
                output, _ = self.model(img1, img2)
                pred = (output.squeeze() > 0.5).float()
                predictions.extend(pred.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())
        
        accuracy = accuracy_score(true_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary', zero_division=0)
        return accuracy, precision, recall, f1
        
    def save_model(self, path):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_accuracy': self.best_accuracy
        }, path)

def train_yolo_model(dataset_path, epochs=30, img_size=640):
    print(f"Starting YOLOv8 training for {epochs} epochs...")
    os.environ['RAY_DISABLE_IMPORT_WARNING'] = '1'

    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        print(f"Error: Dataset configuration file not found at {dataset_path}")
        return None
    
    try:
        with open(dataset_path, 'r') as f:
            config = yaml.safe_load(f)
        print(f"Dataset config: {config}")
    except Exception as e:
        print(f"Error reading dataset.yaml: {e}")
        return None
    
    # Validate dataset paths
    for split in ['train', 'val', 'test']:
        split_path = Path(config['path']) / config[split]
        if not split_path.exists():
            print(f"Error: {split.capitalize()} path {split_path} does not exist!")
            return None
        
        # Check for images
        img_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
        img_files = []
        for ext in img_extensions:
            img_files.extend(list(split_path.glob(ext)))
        
        img_count = len(img_files)
        
        # Check for corresponding labels
        labels_path = split_path.parent / 'labels'
        if labels_path.exists():
            label_files = list(labels_path.glob('*.txt'))
            label_count = len(label_files)
        else:
            label_count = 0
        
        print(f"{split.capitalize()} set: {img_count} images, {label_count} labels")
        
        if img_count == 0:
            print(f"Error: No images found in {split} set!")
            return None
            
        if split != 'val' and label_count == 0:  # val can have no labels
            print(f"Error: No labels found in {split} set!")
            return None
        
        # Show sample files
        if img_count > 0:
            sample_imgs = sorted(img_files)[:3]
            print(f"Sample {split} images: {[f.name for f in sample_imgs]}")
            
        if label_count > 0:
            sample_labels = sorted(label_files)[:3]
            print(f"Sample {split} labels: {[f.name for f in sample_labels]}")
    
    try:
        from ultralytics import YOLO
        from ultralytics.utils import callbacks

        callbacks.default_callbacks = [cb for cb in callbacks.default_callbacks if 'ray' not in str(cb)]

        import ultralytics
        print(f"Ultralytics version: {ultralytics.__version__}")
        settings.update({'datasets_dir': '/kaggle/working', 'wandb': False})
        print(f"Updated Ultralytics datasets_dir to: {settings['datasets_dir']}")
    except ImportError:
        print("Installing ultralytics...")
        subprocess.run(["pip", "install", "ultralytics"], check=True)
        from ultralytics import YOLO
        import ultralytics
        print(f"Ultralytics version: {ultralytics.__version__}")
        settings.update({'datasets_dir': '/kaggle/working', 'wandb': False})
        print(f"Updated Ultralytics datasets_dir to: {settings['datasets_dir']}")
    
    print(f"Training on device: {device}")
    if device.type == "cuda":
        print(f"GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"CUDA Version: {torch.version.cuda}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("Training on CPU")
    
    try:
        model = YOLO("yolov8n.pt")
        print("YOLO model initialized with yolov8n.pt")
        
        print("Starting training...")
        results = model.train(
            data=dataset_path,
            epochs=epochs,
            imgsz=img_size,
            batch=32,
            name="signature_detection",
            cache=False,
            device=device,
            workers=4,
            verbose=True,
            patience=10,
            save=True,
            save_period=1,
            project="/kaggle/working/yolo_project_logs",
            exist_ok=True,
            pretrained=True,
            optimizer='AdamW',
            lr0=0.01,
            lrf=0.001,
            momentum=0.937,
            weight_decay=0.0005,
            warmup_epochs=3.0,
            warmup_momentum=0.8,
            warmup_bias_lr=0.1,
            box=7.5,
            cls=0.5,
            dfl=1.5,
            pose=12.0,
            kobj=1.0,
            label_smoothing=0.0,
            nbs=64,
            hsv_h=0.015,
            hsv_s=0.7,
            hsv_v=0.4,
            degrees=0.0,
            translate=0.1,
            scale=0.5,
            shear=0.0,
            perspective=0.0,
            flipud=0.0,
            fliplr=0.5,
            mosaic=1.0,
            mixup=0.0,
            copy_paste=0.0,
            conf=0.6
        )
        print("Training completed successfully!")
        print(f"Training results: {results}")
    except Exception as e:
        print(f"Error during YOLO training: {str(e)}")
        traceback.print_exc()
        return None
    
    best_model_path = Path("/kaggle/working/yolo_project_logs/signature_detection/weights/best.pt")
    if best_model_path.exists():
        print(f"YOLO model saved at: {best_model_path}")
        return str(best_model_path)
    else:
        print("Error: Best model weights not found!")
        return None

def test_and_visualize_yolo():
    print("Testing and visualizing YOLO model...")
    yolo_model_path = "/kaggle/working/yolo_project_logs/signature_detection/weights/best.pt"
    if not Path(yolo_model_path).exists():
        print("YOLO model not found!")
        return
    
    model = YOLO(yolo_model_path)
    test_images_dir = Path("/kaggle/working/test/images")
    test_images = list(test_images_dir.glob("*"))[:5]
    
    print("Visualizing YOLO detection...")
    for i, img_path in enumerate(test_images):
        try:
            results = model(img_path)
            result_img = results[0].plot()
            cv2.imwrite(f"/kaggle/working/yolo_test_{i}.jpg", result_img)
            print(f"YOLO detection result saved: /kaggle/working/yolo_test_{i}.jpg")
        except Exception as e:
            print(f"Error processing test image {img_path}: {e}")

def test_and_visualize_siamese():
    print("Testing Siamese verification...")
    siamese_model_path = "/kaggle/working/models/best_siamese_model.pth"
    if not Path(siamese_model_path).exists():
        print("Siamese model not found!")
        return
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    siamese_model = SiameseNetwork(input_dim=224)
    checkpoint = torch.load(siamese_model_path, map_location=device)
    siamese_model.load_state_dict(checkpoint['model_state_dict'])
    siamese_model.to(device)
    siamese_model.eval()
    
    ref_dir = Path("/kaggle/working/processed_cedar/reference_signatures")
    ref_images = list(ref_dir.glob("*.png"))[:3]
    pairs_csv = "/kaggle/working/processed_cedar/siamese_data/signature_pairs.csv"
    if not Path(pairs_csv).exists():
        print(f"Pairs CSV not found: {pairs_csv}")
        return
    test_dataset = SignaturePairsDataset(pairs_csv, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)
    
    for i, ref_img_path in enumerate(ref_images):
        try:
            img1 = Image.open(ref_img_path).convert('L')
            img2 = Image.open(ref_img_path).convert('L')
            
            img1_tensor = transform(img1).unsqueeze(0).to(device)
            img2_tensor = transform(img2).unsqueeze(0).to(device)
            
            with torch.no_grad():
                output, distance = siamese_model(img1_tensor, img2_tensor)
                similarity = output.item()
            
            print(f"Siamese test {i}: Similarity = {similarity:.4f}")
        except Exception as e:
            print(f"Error in Siamese test {i}: {e}")
    
    accuracy, precision, recall, f1 = SiameseTrainer(siamese_model, device).test(test_loader)
    print(f"Siamese Test Metrics: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

def main_training_pipeline():
    print("SIGNATURE DETECTION & VERIFICATION TRAINING PIPELINE")
    
    # Prepare paths
    input_dataset_path = Path("/kaggle/input/scanned-signed-documents-for-detection/Scanned Documents Dataset")
    train_img_dir = input_dataset_path / "train" / "img"
    train_json_dir = input_dataset_path / "train" / "ann"
    test_img_dir = input_dataset_path / "test" / "img"
    test_json_dir = input_dataset_path / "test" / "ann"
    
    # Create YOLO directory structure
    yolo_base_dir = Path("/kaggle/working")
    train_images_dir = yolo_base_dir / "train" / "images"
    train_labels_dir = yolo_base_dir / "train" / "labels"
    test_images_dir = yolo_base_dir / "test" / "images"
    test_labels_dir = yolo_base_dir / "test" / "labels"
    
    # Create directories
    for dir_path in [train_images_dir, train_labels_dir, test_images_dir, test_labels_dir]:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    print("Converting JSON annotations to YOLO format...")
    # Convert JSON annotations to YOLO format
    convert_json_to_yolo_format(train_json_dir, train_img_dir, train_labels_dir)
    convert_json_to_yolo_format(test_json_dir, test_img_dir, test_labels_dir)
    
    print("Copying images to YOLO structure...")
    # Copy images to YOLO structure
    img_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
    
    # Copy train images
    for img_file in train_img_dir.iterdir():
        if img_file.suffix.lower() in img_extensions:
            shutil.copy2(img_file, train_images_dir)
    
    # Copy test images
    for img_file in test_img_dir.iterdir():
        if img_file.suffix.lower() in img_extensions:
            shutil.copy2(img_file, test_images_dir)
    
    print(f"Copied {len(list(train_images_dir.glob('*')))} train images")
    print(f"Copied {len(list(test_images_dir.glob('*')))} test images")
    print(f"Created {len(list(train_labels_dir.glob('*.txt')))} train labels")
    print(f"Created {len(list(test_labels_dir.glob('*.txt')))} test labels")
    
    # Create dataset.yaml
    config = {
        'path': str(yolo_base_dir.absolute()),
        'train': 'train/images',
        'val': 'train/images',  # Using train for validation since no separate val set
        'test': 'test/images',
        'nc': 1,
        'names': ['signature']
    }
    
    yaml_path = yolo_base_dir / "dataset.yaml"
    with open(yaml_path, 'w') as f:
        yaml.dump(config, f)
    print(f"YOLO dataset YAML saved at: {yaml_path}")
    
    # Train YOLO model
    yolo_model_path = train_yolo_model(yaml_path, epochs=30, img_size=640)
    if yolo_model_path:
        print(f"YOLO model saved at: {yolo_model_path}")
    else:
        print("YOLO training failed!")
        return
    
    # Process CEDAR for Siamese network
    processor = CEDARDatasetProcessor()
    processor.process_cedar_dataset()
    
    print("Training Siamese network for signature verification...")
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomRotation(10),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    pairs_csv = "/kaggle/working/processed_cedar/siamese_data/signature_pairs.csv"
    if not Path(pairs_csv).exists():
        print(f"Pairs CSV not found: {pairs_csv}")
        return
    
    dataset = SignaturePairsDataset(pairs_csv, transform=transform)
    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Test samples: {len(test_dataset)}")
    
    model = SiameseNetwork(input_dim=224)
    trainer = SiameseTrainer(model, device=device, patience=5)
    
    trainer.train(train_loader=train_loader, val_loader=val_loader, epochs=30, save_dir="models")
    
    # Test and visualize models
    test_and_visualize_yolo()
    test_and_visualize_siamese()
    
    print("TRAINING PIPELINE COMPLETED SUCCESSFULLY!")

if __name__ == "__main__":
    main_training_pipeline()
